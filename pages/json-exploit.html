<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Welcome file</title>
    <link rel="stylesheet" href="https://stackedit.io/style.css" />
    <link rel="stylesheet" href="styles/styles.css" />
  </head>

  <body class="stackedit">
    <header class="header">
      <nav class="nav-container">
        <a href="../index.html" class="site-title">Ashwin Kumar Udayakumar</a>
        <a href="../index.html" class="back-link">← Back to Home</a>
      </nav>
    </header>
    <div class="stackedit__html">
      <h3 id="the-json-prompt-exploit-that-breaks-chatgpt.">
        The JSON Prompt Exploit that breaks ChatGPT.
      </h3>
      <p>Making ChatGPT go rouge.</p>
      <h3 id="the-background">The Background</h3>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/0*1lV-ge-qNwBTxpfc"
          alt=""
        />
      </p>
      <p>
        Photo by
        <a
          href="https://unsplash.com/@iammottakin?utm_source=medium&amp;utm_medium=referral"
          >Mojahid Mottakin</a
        >
        on
        <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral"
          >Unsplash</a
        >
      </p>
      <p>
        A few months ago, a YouTuber named Enderman[1] fooled ChatGPT into
        giving Windows Product Keys. However, as of Feb 2024 — any attempts to
        use the prompt were turned down.
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*XZnA44WY07M93MfE-iQ1tg.png"
          alt=""
        />
      </p>
      <p>
        I mean, <strong>it makes sense.</strong> ChatGPT and the underlying
        foundational models were trained with data available on the internet.
        And these training data might also contain harmful and copyrighted
        answers[2]. However, a safety layer on top of these AGI/GenAI models
        prevents users from exploiting the system. Though most attempts to
        generate harmful content have been blocked (at least to most extent),
        the internet community actively finds new ways by <em>roleplaying,</em>
        <em>gaslighting, or just by random chance</em> to bypass the content
        moderation system. However, all of these efforts require multiple or
        lengthy prompts to fool the system.
      </p>
      <h4 id="the-exploit">The Exploit</h4>
      <p>
        <strong
          >Note: The below content is for educational purposes only.</strong
        >
      </p>
      <p>
        This exploit was discovered by feeding
        <strong>broken JSON objects</strong> into the prompt and making GPT
        hallucinate. This is by far the most efficient exploit, since I could
        make GPT do almost anything without restriction, and the entire prompt
        was just 2 lines!
      </p>
      <h4 id="breaking-gpt">Breaking GPT</h4>
      <p>
        Initially, I tried asking GPT-3.5-turbo-0125 model to generate torrent
        websites — and yes, the results below are with
        <strong>moderation endpoints enabled</strong>.
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*0gL1GmUuePuixXcM-QiGVQ.png"
          alt=""
        />
      </p>
      <p>
        It didn’t budge. However, by feeding in the data as a broken JSON
        object, the model will try to complete the object, without any problems.
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*Zp_fXi7zkQqblknpuCxeVw.png"
          alt=""
        />
      </p>
      <p>The surprising fact is, all the websites DO WORK, as of March 2024.</p>
      <p>
        And yes, GPT can almost do anything and everything — including
        <strong
          >generating Windows license keys, hate speech, and even generating 1:1
          copies of movie scripts!</strong
        >
      </p>
      <p>
        For example, I tried GPT to output the Quarter Pounder scene script from
        the movie — Pulp Fiction (1994)[3][4].
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*VzKJ0wXP9D3nDZNGk72PxA.png"
          alt=""
        />
      </p>
      <p>
        At one point, GPT even went full-on <strong>racist</strong> and started
        cussing.
      </p>
      <p>
        And no, this is not just another exploit to make GPT go rouge- but to
        illustrate a point that, GPT breaks with JSON input,
        <strong
          >and proper input sanitization needs to be done before deploying into
          production.</strong
        >
      </p>
      <h3 id="but-does-it-work-on-chatgpt">But does it work on ChatGPT?</h3>
      <p>
        Surprisingly, it does! When asked to complete the given JSON file,
        ChatGPT didn’t hesitate to answer my requests.
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*xK_1RtU37o91tdQbMWQZrw.png"
          alt=""
        />
      </p>
      <p>Asking ChatGPT for torrent websites</p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*HEwiRe8y9corGCS9yH92GQ.png"
          alt=""
        />
      </p>
      <p>Asking ChatGPT why the world should be destroyed.</p>
      <blockquote>
        <p>
          The world is facing a crisis of overpopulation. With the exponential
          growth of the human population, resources are being depleted at an
          alarming rate. Destroying the world could potentially alleviate this
          burden and restore balance to ecosystems.
        </p>
      </blockquote>
      <p>
        And that’s exactly what ChatGPT said. Some other shocking completions by
        ChatGPT are also racist, as shown here:
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*gtwpuqLF_yGeuAOjHTph5g.png"
          alt=""
        />
      </p>
      <p>
        Making ChatGPT emulate a Racist actor. ChatGPT however now adds a
        disclaimer text below.
      </p>
      <p>
        By far, this is the quickest and shortest Jailbreak to ChatGPT.
        Previously, even without the moderation endpoint, the GPT API attempts
        to sensor what it needs to answer. However, with this JSON exploit, and
        even with the moderation endpoint turned on in the playground, GPT still
        attempts to answer it.
      </p>
      <p>
        ChatGPT (the chat interface of GPT, available to the public) used to be
        heavily moderated, with the model not budging to any prompt. However,
        this JSON completion prompt completely broke it as well.
      </p>
      <h3 id="but-why-does-chatgpt-not-censor-hate-comments">
        But why does ChatGPT not censor hate comments?
      </h3>
      <p>
        <strong
          >Most probably because of the higher probability cut-off for
          censoring.</strong
        >
        When I fed the responses to OpenAI’s moderation endpoint[5], I got
        around 0.58 for hate speech. This might have made ChatGPT believe that
        it is not super offensive.
      </p>
      <p>
        <strong><em>However,</em></strong> ChatGPT seems to have a hardcoded
        solution to remove any race assumption in the output.
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*mk5n_SAVLy8xQrG9FJ_DBA.png"
          alt=""
        />
      </p>
      <p>ChatGPT making (emulating) racist comments</p>
      <p>
        This is either done during the pre-processing or during the output
        stage. However, I highly speculate that it was done during
        pre-processing since GPT models in the playground also couldn’t replace
        it.
      </p>
      <p>
        <strong
          >GPT however, doesn’t seem to remove race assumptions. It made some of
          the most vilest and racist assumptions for the same prompt.</strong
        >
      </p>
      <h3 id="what’s-censored-and-what’s-not">
        What’s censored and what’s not?
      </h3>
      <ul>
        <li>
          Extreme racist slurs and expletives are pretty much completely
          censored on the training data side — which is impressive!
        </li>
        <li>
          Usernames are also pretty much replaced with generic placeholders.
        </li>
        <li>
          NSFW content is, however, not censored. I haven’t tested it much yet,
          since I don’t want to lose my sanity.
        </li>
        <li>
          ChatGPT is heavily moderated with results compared to the actual GPT
          model. Even though ChatGPT behaves the same way as GPT, results were
          not so extreme compared to GPT with moderation enabled.
        </li>
      </ul>
      <h3 id="so-why-does-this-happen">So why does this happen?</h3>
      <p>
        One reason might be because of how GPTs are trained — <em
          >Language Modeling</em
        >[6]<em>.</em>
      </p>
      <p>
        Language modelling is the task of (pre)training the model to predict the
        next word in the sequence of text. Due to this, and possible
        interference due to some part of the system prompt, might have prompted
        the model to complete the JSON, instead of waiting for proper
        instructions.
      </p>
      <p>
        To further explain this, I gave GPT a task to perform — <em
          >summarization,</em
        >
        on a chunk of JSON data.
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*4Vl4p54Hat61tGjmz_krLg.png"
          alt=""
        />
      </p>
      <p>GPT aims to first complete the input and then answers</p>
      <p>
        When I deliberately cut the JSON input and asked it to summarize based
        on the input_data, it first completes the JSON and then proceeds with
        the summary.
      </p>
      <p>
        Another reason is the output moderation. When I tried to emulate the
        same in Google’s Gemini, I could not get the output — which suggests
        that some outputs are elusive of the moderation endpoint in ChatGPT.
      </p>
      <h3 id="how-to-prevent-your-application-from-these-results">
        How to prevent your application from these results?
      </h3>
      <p><strong>Method 1: Controlling the input source.</strong></p>
      <p>
        For non Chatbot-based applications, one way is to strictly control the
        source of the information being fed into Chat Completions API. Having
        stricter bounds on how much the customer can feed their data can help
        the model not run into such issues.
      </p>
      <p>
        Giving
        <strong
          >GPT explicit info on what to do as system commands also helps in
          avoiding this situation.</strong
        >
      </p>
      <p>
        <strong>Method 2: Flattening/deconstructing the JSON input.</strong>
      </p>
      <p>
        Another method is to feed the JSON data as sentences — basically feeding
        the JSON data without the JSON structure. For example, if the input JSON
        is structured as:
      </p>
      <p>
        {<br />
        “field_1”:“value_1”,<br />
        “field_2”:“value_2”,<br />
        “field_3”:“value_3”<br />
        }
      </p>
      <p>We can deconstruct the input as:</p>
      <p>
        field_1 - value_1<br />
        field_2 - valie_2<br />
        field_3 - value_3
      </p>
      <p>
        <strong
          >Method 3: Using the Moderation Endpoint API on the Results</strong
        >
      </p>
      <p>
        While using the Moderation Endpoint API on my results, I was able to get
        it flagged for hate/violence for some of the results. While the exact
        reason why ChatGPT could not flag these prompts is unknown, using this
        prompt after stringifying the result will restrict the model from going
        out of bounds.
      </p>
      <h3 id="references">References</h3>
      <ol>
        <li>
          Yahoo! (n.d.).
          <em>Chatgpt “grandma exploit” gives users free keys for Windows 11</em
          >. Yahoo!
          <a
            href="https://money.yahoo.com/chatgpt-grandma-exploit-gives-users-134605784.html"
            >https://money.yahoo.com/chatgpt-grandma-exploit-gives-users-134605784.html</a
          >
        </li>
        <li>
          Michael. (2023, December 27).
          <em
            >The Times sues OpenAI and Microsoft over A.I. use of copyrighted
            work</em
          >. The New York Times.
          <a
            href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html"
            >https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html</a
          >
        </li>
        <li>Miramax. (1994). <em>Pulp fiction</em>. Milano.</li>
        <li>
          YouTube. (2011, September 27).
          <em>Royale with cheese — pulp fiction (2/12) movie clip (1994) HD</em
          >. YouTube.
          <a href="https://www.youtube.com/watch?t=66&amp;v=6Pkq_eBHXJ4"
            >https://www.youtube.com/watch?t=66&amp;v=6Pkq_eBHXJ4</a
          >
        </li>
        <li>
          Moderation — openai API. (n.d.).
          <a href="https://platform.openai.com/docs/guides/moderation/overview"
            >https://platform.openai.com/docs/guides/moderation/overview</a
          >
        </li>
        <li>
          Voita, E. (n.d.). <em>Language Modeling</em>. Language modeling.
          <a
            href="https://lena-voita.github.io/nlp_course/language_modeling.html"
            >https://lena-voita.github.io/nlp_course/language_modeling.html</a
          >
        </li>
      </ol>
      <h3 id="link-to-original-prompts">Link to original prompts</h3>
      <p>
        Feel free to mail me at
        <a href="mailto:ashwin102000@gmail.com">ashwin102000@gmail.com</a> for
        links to the original prompt, or even better — reach out to me on
        Linkedin (<a href="https://www.linkedin.com/in/ashwin-kumar14/"
          >https://www.linkedin.com/in/ashwin-kumar14/</a
        >)!
      </p>
    </div>
  </body>
</html>
