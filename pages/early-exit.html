<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Welcome file</title>
    <link rel="stylesheet" href="https://stackedit.io/style.css" />
    <link rel="stylesheet" href="styles/styles.css" />
  </head>

  <body class="stackedit">
    <header class="header">
      <nav class="nav-container">
        <a href="../index.html" class="site-title">Ashwin Kumar Udayakumar</a>
        <a href="../index.html" class="back-link">← Back to Home</a>
      </nav>
    </header>
    <div class="stackedit__html">
      <h3 id="early-exiting-for-image-captioning-models">
        Early Exiting for Image Captioning Models
      </h3>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*cVgBdLpgyQ4GGTUnql5-Pw.jpeg"
          alt=""
        />
      </p>
      <p>It’s all about speed!</p>
      <p>
        <strong><em>But first, — WHAT IS EARLY EXITING?</em></strong>
      </p>
      <p>
        As described in this
        <a href="https://arxiv.org/abs/1709.01686v1">paper</a> as “BranchyNet”,
        these neural networks have exit branches at specific locations
        throughout the network.
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*7S3WYNcb8iuGGx9rDWgaUQ.png"
          alt=""
        />
      </p>
      <p>
        Source:
        <a href="https://arxiv.org/pdf/1709.01686v1.pdf"
          >https://arxiv.org/pdf/1709.01686v1.pdf</a
        >
      </p>
      <h3 id="what’s-the-main-idea-of-these-branches-then">
        <strong>What’s the main idea of these branches then?</strong>
      </h3>
      <p>
        Suppose you have 100 layers in a neural network. After passing the input
        image through the input layer, it must go through all layers to generate
        an output.
        <strong
          >But, you realize that for some samples, you can classify them
          accurately after just passing through <em>n</em> layers (for example,
          5 layers).</strong
        >
        In this case, feed-forward passes through all other layers is wasteful
        in terms of computational resources. So, we exit at the
        <em>nth</em> layer through a branch.
      </p>
      <h4
        id="this-is-especially-useful-in-large-networks-that-have-100s-of-layers-in-them."
      >
        <strong
          >This is especially useful in large networks that have 100s of layers
          in them.</strong
        >
      </h4>
      <p>
        It even allows large networks like AlexNet to be deployed for real-time
        inferences. But when it comes to Image captioning models, there’s a
        <strong><em>catch</em></strong
        >.
      </p>
      <h3 id="image-captioning-models">Image Captioning Models</h3>
      <p>
        Image captioning models are different. They are basically
        **Encoder-Decoder models — **I won’t go deep into the Encoder-Decoder
        architecture, but on the surface level, the encoder part creates a
        representation/intermediate vector based on the input, and the decoder
        predicts the token(word) at each timestep.
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*UvACBSAvc_4YrKq92mzTMQ.png"
          alt=""
        />
      </p>
      <p>
        Source:
        <a
          href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fei_DeeCap_Dynamic_Early_Exiting_for_Efficient_Image_Captioning_CVPR_2022_paper.pdf"
          >https://openaccess.thecvf.com/content/CVPR2022/papers/Fei_DeeCap_Dynamic_Early_Exiting_for_Efficient_Image_Captioning_CVPR_2022_paper.pdf</a
        >
      </p>
      <p>
        Speaking of these encoder-decoder models, can we also apply early-exit
        branches to them?
        <strong><em>Yes, but with some modifications.</em></strong>
      </p>
      <p>
        These “modifications” are made possible, thanks to Fei, Zhengcong, et
        al. and their research work
        <a
          href="https://openaccess.thecvf.com/content/CVPR2022/html/Fei_DeeCap_Dynamic_Early_Exiting_for_Efficient_Image_Captioning_CVPR_2022_paper.html"
          >“DeeCap: Dynamic Early Exiting for Efficient Image Captioning”</a
        >.
      </p>
      <p>
        In this paper, the authors saw that conventional early exiting is not
        possible in the case of <em>encoder-decoder</em> networks, since at the
        shallow level, the representations generated by the decoder layers are
        insufficient to predict tokens accurately, since they lack high-level
        semantic information.
      </p>
      <p>
        That is, let’s say that we are early exiting after <em>n</em> decoder
        layers. The representation produced by those <em>n</em> decoders is not
        sufficient to predict accurately, and we still need the outputs from the
        deeper decoder layers after them.
      </p>
      <p>
        <strong
          >The workaround? Try to replicate the outputs of the deeper decoder
          layers!</strong
        >
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*yK9a0gyL2nnFl_CEoGAGvA.png"
          alt=""
        />
      </p>
      <p>
        Source:
        <a
          href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fei_DeeCap_Dynamic_Early_Exiting_for_Efficient_Image_Captioning_CVPR_2022_paper.pdf"
          >https://openaccess.thecvf.com/content/CVPR2022/papers/Fei_DeeCap_Dynamic_Early_Exiting_for_Efficient_Image_Captioning_CVPR_2022_paper.pdf</a
        >
      </p>
      <p>
        First, let us assume that we are going to exit early at layer
        <em>m.</em> So this means that for layer ‘k’, where k≤m we have done the
        forward pass. And for every layer ‘k’ where k&gt;m, we have to replicate
        the representations.
      </p>
      <p>
        This <strong>replication</strong> is done with the help of imitation
        learning. Let’s say we want to replicate the representation of layer k
        where k&gt;m. The k-th imitation network paired with the k-th layer
        inputs the truly hidden state h_m and outputs:
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/0*UWfSLbbJ9QyqDAyn"
          alt=""
        />
      </p>
      <p>Imitated Representation</p>
      <p>
        where MLP is a multi-layer perception. But these MLPs also need to be
        trained accordingly, so that they can output the imitation
        representations. This is done with the help of cosine similarity as our
        main loss function.
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*PjkyaQ1BLmHN7QjjS9qn9w.png"
          alt=""
        />
      </p>
      <p>Cosine Similarity Loss Function</p>
      <h3
        id="now-we-found-a-way-to-get-the-output-of-the-deeper-decoder-layers-using-imitation-learning-and-how-to-train-the-mlp-that-facilitates-imitation-learning."
      >
        <strong
          >Now, we found a way to get the output of the deeper decoder layers
          using imitation learning, and how to train the MLP that facilitates
          imitation learning.</strong
        >
      </h3>
      <p>
        But we are still a few steps away from achieving early-exiting. Now that
        we have the representations of all the decoder layers, we can group them
        into 2 kinds: <strong>h-shallow</strong> are the layers that performed
        the actual feed-forward, and <strong>h-deep</strong> are the layers that
        used imitation layers.
      </p>
      <p>
        Now, we need to aggregate these h-shallow and h-deep layers. This
        aggregation is done with the help of a
        <strong>fusion strategy.</strong> There different fusion strategies that
        we can use are:
      </p>
      <ol>
        <li>Average Strategy</li>
        <li>Concatenation Strategy</li>
        <li>Attention-Pooling Strategy</li>
        <li>Sequential Strategy</li>
      </ol>
      <p>
        Now that we finished aggregating the h-shallow layers separately, and
        h-deep layers separately, we now need to know how to
        <strong>merge</strong> them.
      </p>
      <h3 id="this-calls-for-a-gate-decision-mechanism.">
        This calls for a Gate Decision Mechanism.
      </h3>
      <p>So far we know these facts:</p>
      <ol>
        <li>
          h-deep layers representations are produced using imitation networks.
        </li>
        <li>
          h-shallow layers representations are produced by the actual
          feed-forward mechanism.
        </li>
      </ol>
      <p>
        Since the h-shallow layers have actually <em>seen</em> the input data,
        they are more likely to be reliable. So, we need to find a
        <strong>balance factor</strong> to control how much information we need
        from h-shallow vs h-deep. This balance factor <strong>α</strong> is
        calculated using the following formula:
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*D3ezmx8Ph0vkaWnnyXdPSw.png"
          alt=""
        />
      </p>
      <p>
        With this balancing factor, we can finally represent the final merged
        information as
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*BwBoGvKHX_Gyvj2ItqNJPg.png"
          alt=""
        />
      </p>
      <p>
        After applying the softmax function to zₘ, we can calculate the output
        class distribution as
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*sKu2dKMsCVD-nEz2rYaDGg.png"
          alt=""
        />
      </p>
      <p>
        But, we also need to dynamically learn the balancing factor. The
        corresponding loss function is:
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*uzlaOkaz86lvROEZlqz79g.png"
          alt=""
        />
      </p>
      <p>Thus, the final training objective is:</p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*s565WG6MYg00e55HhDGO8Q.png"
          alt=""
        />
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*devyv3gEcR0fPd9co_Zq3g.gif"
          alt=""
        />
      </p>
      <p>That’s a lot of equations!</p>
      <h3 id="now-we-can-finally-talk-about-training-and-inference">
        Now, we can finally talk about training and inference!
      </h3>
      <p>
        Remember that we calculated <em>pₘ</em> previously? By calculating the
        entropy of <em>pₘ</em> as H(pₘ), we calculate the prediction confidence
        of the current token at layer m. We then compare the prediction
        confidence with a manually selected threshold τ.
        <strong
          >Once this value is lower than the threshold, we can exit!</strong
        >
      </p>
      <p>
        Also, note that the shallow layer’s weights will be updated more
        frequently because they are receiving more update signals than the deep
        layers. So, while calculating the cross entropy loss of each layer, we
        reweight them using the following equation:
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/1*KgK0bNwwzHWKdSJyxkjtcw.png"
          alt=""
        />
      </p>
      <p>
        Added to this, the authors argue that updating all parameters at the
        same time at each time step will damage the well-trained features during
        fine-tuning. Therefore, they try to freeze the parameters at each layer
        with a probability <em>p</em>, and it linearly decreases from 1 to 0
        with depth.
      </p>
      <p>
        <strong
          >And that’s how we add early-exit to encoder-decoder based
          models!</strong
        >
      </p>
      <p>
        <img
          src="https://cdn-images-1.medium.com/max/1000/0*7235C_rYc-1A0EQU.gif"
          alt=""
        />
      </p>
      <h3 id="references">References:</h3>
      <ol>
        <li>
          Fei, Zhengcong, et al. “DeeCap: Dynamic Early Exiting for Efficient
          Image Captioning.”
          <em
            >Proceedings of the IEEE/CVF Conference on Computer Vision and
            Pattern Recognition</em
          >. 2022.
        </li>
        <li>
          Teerapittayanon, Surat, Bradley McDanel, and Hsiang-Tsung Kung.
          “Branchynet: Fast inference via early exiting from deep neural
          networks.”
          <em
            >2016 23rd International Conference on Pattern Recognition
            (ICPR)</em
          >. IEEE, 2016.
        </li>
      </ol>
    </div>
  </body>
</html>
